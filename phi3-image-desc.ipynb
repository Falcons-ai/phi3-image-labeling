{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Falcons.ai Image labeling using Microsoft Phi-3-Vision\n",
    "### phi3 env\n",
    "### [The microsoft/Phi-3-vision-128k-instruct model](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case:\n",
    "### Tagging a large volume of images in a specific directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import textwrap\n",
    "import warnings\n",
    "from PIL import Image \n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from transformers import AutoProcessor \n",
    "from transformers import AutoModelForCausalLM \n",
    "from IPython.display import display, Markdown, Latex, JSON\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Cuda specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Image_desc(image_path,prompt='Describe this image in excruciating detail'):\n",
    "\n",
    "    ## If you have not downloaded the model:\n",
    "    # model_id = \"microsoft/Phi-3-vision-128k-instruct\" \n",
    "    # model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"auto\", _attn_implementation='flash_attention_2') # use _attn_implementation='eager' to disable flash attention\n",
    "\n",
    "    # I have downloaded the model to a sub-directory titled \"phi3\"\n",
    "    model_id = \"./phi3\" \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"auto\")\n",
    "    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True) \n",
    "\n",
    "    messages = [ \n",
    "        {\"role\": \"user\", \"content\": \"<|image_1|>\\n\"+prompt}, \n",
    "    ] \n",
    "    # I will be using local images\n",
    "    image = Image.open(image_path) \n",
    "    prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(prompt, [image], return_tensors=\"pt\").to(\"cuda:0\") \n",
    "    generation_args = { \n",
    "        \"max_new_tokens\": 500, \n",
    "        \"temperature\": 0.0, \n",
    "        \"do_sample\": False, \n",
    "    } \n",
    "    generate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args) \n",
    "\n",
    "    # remove input tokens \n",
    "    generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "    response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] \n",
    "    plt.imshow(image)\n",
    "    wrapped_title = \"\\n\".join(textwrap.wrap(response, width=40)) \n",
    "    plt.title(wrapped_title)\n",
    "    plt.axis('off')  # Hide the axis for better visualization\n",
    "    plt.show()\n",
    "    return response "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save image description to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image_description(image_path, description):\n",
    "    # Get the base name of the image file without extension\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    # Create a text file name with the same base name\n",
    "    text_file_path = os.path.join(os.path.dirname(image_path), f\"{base_name}.txt\")\n",
    "    \n",
    "    # Write the description to the text file\n",
    "    with open(text_file_path, 'w') as text_file:\n",
    "        text_file.write(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get images in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_jpg_images(directory):\n",
    "    # Initialize an empty list to store image paths\n",
    "    jpg_images = []\n",
    "\n",
    "    # Use glob to find all .jpg files in the directory\n",
    "    jpg_files = glob.glob(os.path.join(directory, '*.*'))\n",
    "    jpg_images.extend(jpg_files)\n",
    "\n",
    "    return jpg_images\n",
    "\n",
    "directory_path = 'images'\n",
    "jpg_images = list_jpg_images(directory_path)\n",
    "# Show unsorted\n",
    "print(jpg_images)\n",
    "jpg_images.sort()\n",
    "# Show sorted\n",
    "print(jpg_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test an individual image with custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'images/test3.jpg'\n",
    "noVar = Image_desc(img_path, 'Deeply describe exverything in and that is happening in this image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loop through images for description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the sorted list\n",
    "for i in jpg_images:\n",
    "    description = Image_desc(i)\n",
    "    save_image_description(i, description)\n",
    "    # Added the sleep function to prevent Cuda memory issues\n",
    "    time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you downloaded the model from Huggingface, and want to run it local going forward\n",
    "Uncomment the code below to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the path where you want to save the model and processor\n",
    "# model_save_path = \"./phi3\"\n",
    "# processor_save_path = \"./phi3\"\n",
    "\n",
    "# # Save the model with safe serialization\n",
    "# model.save_pretrained(model_save_path, safe_serialization=False)\n",
    "\n",
    "# # Save the processor\n",
    "# processor.save_pretrained(processor_save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phi3",
   "language": "python",
   "name": "phi3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
